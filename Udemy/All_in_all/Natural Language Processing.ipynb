{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP\n",
    "\n",
    "Computers to process and analyze large amount of natural data. Computers can read text, hear speech, interpret it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "Online chatbox, Speech Recognition etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bhawindhital/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I believe this would help the reader understand how tokenization         works.', 'as well as realize its importance.']\n",
      "['I', 'believe', 'this', 'would', 'help', 'the', 'reader', 'understand', 'how', 'tokenization', 'works', '.', 'as', 'well', 'as', 'realize', 'its', 'importance', '.']\n",
      "[['I', 'believe', 'this', 'would', 'help', 'the', 'reader', 'understand', 'how', 'tokenization', 'works', '.'], ['as', 'well', 'as', 'realize', 'its', 'importance', '.']]\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "#Import libraries\n",
    "#import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"I believe this would help the reader understand how tokenization \\\n",
    "        works. as well as realize its importance.\"\n",
    "        \n",
    "sents = (sent_tokenize(text))\n",
    "print(sents)\n",
    "print(word_tokenize(text))\n",
    "\n",
    "words = [word_tokenize(sent) for sent in sents]\n",
    "print(words)\n",
    "#perform tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bhawindhital/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "text = \"I believe this would help the reader understand how tokenization \\\n",
    "        works. as well as realize its importance (text) .\"\n",
    "        \n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'believe', 'would', 'help', 'reader', 'understand', 'tokenization', 'works', 'well', 'realize', 'importance', 'text']\n"
     ]
    }
   ],
   "source": [
    "custom_list = set(stopwords.words('english')+list(punctuation))\n",
    "\n",
    "word_list = [word for word in word_tokenize(text) if word not in custom_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(('I', 'believe'), 1), (('believe', 'would'), 1), (('would', 'help'), 1), (('help', 'reader'), 1), (('reader', 'understand'), 1), (('understand', 'tokenization'), 1), (('tokenization', 'works'), 1), (('works', 'well'), 1), (('well', 'realize'), 1), (('realize', 'importance'), 1), (('importance', 'text'), 1)])\n"
     ]
    }
   ],
   "source": [
    "#N-grams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "word_list = ['I', 'believe', 'would', 'help', 'reader', 'understand', \\\n",
    "             'tokenization', 'works', 'well', 'realize', 'importance', 'text']\n",
    "\n",
    "finde = BigramCollocationFinder.from_words(word_list)\n",
    "print(finde.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'import', 'to', 'by', 'very', 'python', 'whil', 'you', 'ar', 'python', 'with', 'python', '.', 'al', 'python', 'hav', 'python', 'poor', 'at', 'least', 'ont', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "#Import Libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "l_s = LancasterStemmer()\n",
    "new_text = \"It is important to by very pythonly while you are pythoning\\\n",
    "             with python. All pythoners have pythoned poorly at least once.\"\n",
    "             \n",
    "stem_lan =  [l_s.stem(word) for word in word_tokenize(new_text)] \n",
    "print(stem_lan)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bhawindhital/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('mouse.n.01') any of numerous small rodents typically resembling diminutive rats having pointed snouts and small ears on elongated bodies with slender usually hairless tails\n",
      "Synset('shiner.n.01') a swollen bruise caused by a blow to the eye\n",
      "Synset('mouse.n.03') person who is quiet or timid\n",
      "Synset('mouse.n.04') a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n",
      "Synset('sneak.v.01') to go stealthily or furtively\n",
      "Synset('mouse.v.02') manipulate the mouse of a computer\n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "Synset('mouse.n.04') a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n"
     ]
    }
   ],
   "source": [
    "#Word Sense Disambiguation\n",
    "#Import Libraries\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "for ss in wordnet.synsets('mouse'):\n",
    "    print(ss, ss.definition())\n",
    "\n",
    "\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "context_1 = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"), \"bass\")\n",
    "print(context_1, context_1.definition())\n",
    "\n",
    "context_2 = lesk(word_tokenize(\"The sea bass really very hard to catch\"), \"bass\")\n",
    "print(context_2, context_2.definition())\n",
    "\n",
    "context_3 = lesk(word_tokenize(\"My mouse is not working, need to change it\"), \"mouse\")\n",
    "print(context_3, context_3.definition())\n",
    "\n",
    "\n",
    "#\"Sing in a lower tone, along with the bass\"\n",
    "#\"The sea bass really very hard to catch\"\n",
    "#\"My mouse is not working, need to change it\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the first document from heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>but the second document is from mars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And this is the third one from nowhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this the first document from nowhere?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text\n",
       "0    This is the first document from heaven\n",
       "1      but the second document is from mars\n",
       "2    And this is the third one from nowhere\n",
       "3  Is this the first document from nowhere?"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corpus = [\n",
    "     'This is the first document from heaven',\n",
    "     'but the second document is from mars',\n",
    "     'And this is the third one from nowhere',\n",
    "     'Is this the first document from nowhere?',\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'text':corpus})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 1 0 0 0 0 1 0 1]\n",
      " [0 1 1 0 1 0 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 0 1 0 1 0 1 1 0 1 1 1]\n",
      " [0 0 1 1 1 0 1 0 1 0 0 1 0 1]]\n",
      "{'this': 13, 'is': 6, 'the': 11, 'first': 3, 'document': 2, 'from': 4, 'heaven': 5, 'but': 1, 'second': 10, 'mars': 7, 'and': 0, 'third': 12, 'one': 9, 'nowhere': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_v = CountVectorizer()\n",
    "X = count_v.fit_transform(df.text).toarray()\n",
    "print(X)\n",
    "print(count_v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 0 0 0 0 1 0]\n",
      " [0 1 1 0 1 0 1 0 0 1 1 0]\n",
      " [1 0 0 0 1 0 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 0 0 1 0 0 1 0]]\n",
      "{'the': 10, 'first': 3, 'document': 2, 'from': 4, 'heaven': 5, 'but': 1, 'second': 9, 'mars': 6, 'and': 0, 'third': 11, 'one': 8, 'nowhere': 7}\n"
     ]
    }
   ],
   "source": [
    "count_v = CountVectorizer(stop_words=['this','is'])\n",
    "X = count_v.fit_transform(df.text).toarray()\n",
    "print(X)\n",
    "print(count_v.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 13, 'is': 6, 'the': 11, 'first': 3, 'document': 2, 'from': 4, 'heaven': 5, 'but': 1, 'second': 10, 'mars': 7, 'and': 0, 'third': 12, 'one': 9, 'nowhere': 8}\n",
      "[1.91629073 1.91629073 1.22314355 1.51082562 1.         1.91629073\n",
      " 1.         1.91629073 1.51082562 1.91629073 1.91629073 1.\n",
      " 1.91629073 1.22314355]\n"
     ]
    }
   ],
   "source": [
    "#TD-IDF\n",
    "#Import Libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "     'This is the first document from heaven',\n",
    "     'but the second document is from mars',\n",
    "     'And this is the third one from nowhere',\n",
    "     'Is this the first document from nowhere?',\n",
    "]\n",
    "\n",
    "vector = TfidfVectorizer()\n",
    "vector.fit(corpus)\n",
    "print(vector.vocabulary_)\n",
    "print(vector.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 0., 0., 1., 1., 2., 0.],\n",
       "       [2., 0., 0., 1., 1., 1., 2., 0.],\n",
       "       [0., 0., 0., 0., 2., 3., 3., 0.],\n",
       "       [2., 0., 0., 0., 1., 1., 3., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hashing\n",
    "#Import Libraries\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import pandas as pd\n",
    "corpus = [\n",
    "     'This is the first document from heaven',\n",
    "     'but the second document is from mars',\n",
    "     'And this is the third one from nowhere',\n",
    "     'Is this the first document from nowhere?',\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'text':corpus})\n",
    "\n",
    "hash_v = HashingVectorizer(n_features=8, norm=None,alternate_sign=False)\n",
    "hash_v.fit_transform(df.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
